{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\n",
    "from pyspark.sql.functions import col,regexp_extract,concat,lit,array,unix_timestamp,when,row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "import json \n",
    "from pyspark.ml.feature import StringIndexer,StringIndexerModel\n",
    "from pyspark.conf import SparkConf\n",
    "from rule_base.rules import Apache_error\n",
    "\n",
    "import time\n",
    "\n",
    "### For Phase 4 ###\n",
    "from pyspark.sql.functions import from_unixtime, col, unix_timestamp, expr,lag,round,dayofweek,length,regexp_replace,count, desc,udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[\n",
    "'{\"time\":\"Jan 21 00:01:31\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"reply 3x6-.601-.TtfFB8j218UXwO8pVc68oxPnxxXW/9Otff-.Sipro*dGQOOMDr0IY1r2WRpfKdpesdSo9G-.i0rOSwEM5h8DYTFzpxaiOTWPKRKJaPZ4Yt-.ynQPRqKPCFFdlw6fN6NAz3VT516DMTVkCG-.customers_2017.xlsx.email-19.kennedy-mendoza.info is 195.128.194.168\"}',\n",
    "'{\"time\":\"Jan 21 00:01:44\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"query[A] 3x6-.602-.OgEWPzRasaBfzw*1YgPwvfpK5ndYxfeL6t-.AAWCJrJfzqWHDvNtdSStYg3cnHaa0NHOxM-.26JBAO6h3xFvABN5REohh3RObsbew4lIQs-.XDxHdPvCAsmEqutMHFanKav*p1u36CvjZ4-.customers_2017.xlsx.email-19.kennedy-mendoza.info from 10.143.0.103\"}',\n",
    "'{\"time\":\"Jan 21 00:01:44\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"forwarded 3x6-.602-.OgEWPzRasaBfzw*1YgPwvfpK5ndYxfeL6t-.AAWCJrJfzqWHDvNtdSStYg3cnHaa0NHOxM-.26JBAO6h3xFvABN5REohh3RObsbew4lIQs-.XDxHdPvCAsmEqutMHFanKav*p1u36CvjZ4-.customers_2017.xlsx.email-19.kennedy-mendoza.info to 192.168.231.254\"}',\n",
    "'{\"time\":\"Jan 21 00:01:44\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"reply 3x6-.602-.OgEWPzRasaBfzw*1YgPwvfpK5ndYxfeL6t-.AAWCJrJfzqWHDvNtdSStYg3cnHaa0NHOxM-.26JBAO6h3xFvABN5REohh3RObsbew4lIQs-.XDxHdPvCAsmEqutMHFanKav*p1u36CvjZ4-.customers_2017.xlsx.email-19.kennedy-mendoza.info is 195.128.194.168\"}',\n",
    "'{\"time\":\"Jan 21 00:02:03\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"query[A] 3x6-.603-.JAiihufiH6QIEbwkSuO95WxdELeD1lhhZM-.d9flhBVmZjVw*hHY4zQbeBvDhO7Rkx310e-.qpg3bNc2PfFMkhRJ5fHiwXm/77ZJqwhnpT-.wB1zoS/YSaMDXtmgJxN1OFHSuQqg51iWtQ-.customers_2017.xlsx.email-19.kennedy-mendoza.info from 10.143.0.103\"}',\n",
    "'{\"time\":\"Jan 21 00:02:03\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"forwarded 3x6-.603-.JAiihufiH6QIEbwkSuO95WxdELeD1lhhZM-.d9flhBVmZjVw*hHY4zQbeBvDhO7Rkx310e-.qpg3bNc2PfFMkhRJ5fHiwXm/77ZJqwhnpT-.wB1zoS/YSaMDXtmgJxN1OFHSuQqg51iWtQ-.customers_2017.xlsx.email-19.kennedy-mendoza.info to 192.168.231.254\"}',\n",
    "'{\"time\":\"Jan 21 00:02:03\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"reply 3x6-.603-.JAiihufiH6QIEbwkSuO95WxdELeD1lhhZM-.d9flhBVmZjVw*hHY4zQbeBvDhO7Rkx310e-.qpg3bNc2PfFMkhRJ5fHiwXm/77ZJqwhnpT-.wB1zoS/YSaMDXtmgJxN1OFHSuQqg51iWtQ-.customers_2017.xlsx.email-19.kennedy-mendoza.info is 195.128.194.168\"}',\n",
    "'{\"time\":\"Jan 21 00:02:18\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"query[A] 3x6-.604-.MjQIsKRLJeIf3y1wM8tvXuGtFrV3H9WyLp-.FM74lLN5B5lFdxKv/oKEQF1IcKHe4qnp15-.myRWf8hQktgRZYoYMN84ec*T7Tx4Cu0F*5-.Atajwu7v2GXv2WXGRPV6jPKi6tEHcp72sD-.customers_2017.xlsx.email-19.kennedy-mendoza.info from 10.143.0.103\"}',\n",
    "'{\"time\":\"Jan 21 00:02:18\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"forwarded 3x6-.604-.MjQIsKRLJeIf3y1wM8tvXuGtFrV3H9WyLp-.FM74lLN5B5lFdxKv/oKEQF1IcKHe4qnp15-.myRWf8hQktgRZYoYMN84ec*T7Tx4Cu0F*5-.Atajwu7v2GXv2WXGRPV6jPKi6tEHcp72sD-.customers_2017.xlsx.email-19.kennedy-mendoza.info to 192.168.231.254\"}',\n",
    "'{\"time\":\"Jan 21 00:02:18\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"reply 3x6-.604-.MjQIsKRLJeIf3y1wM8tvXuGtFrV3H9WyLp-.FM74lLN5B5lFdxKv/oKEQF1IcKHe4qnp15-.myRWf8hQktgRZYoYMN84ec*T7Tx4Cu0F*5-.Atajwu7v2GXv2WXGRPV6jPKi6tEHcp72sD-.customers_2017.xlsx.email-19.kennedy-mendoza.info is 195.128.194.168\"}',\n",
    "'{\"time\":\"Jan 21 00:02:27\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"query[TXT] current.cvd.clamav.net from 172.19.130.4\"}',\n",
    "'{\"time\":\"Jan 21 00:02:27\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"forwarded current.cvd.clamav.net to 192.168.231.254\"}',\n",
    "'{\"time\":\"Jan 21 00:02:27\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"reply current.cvd.clamav.net is 0.103.5:62:26428:1642717740:1:90:49192:333\"}',\n",
    "'{\"time\":\"Jan 21 00:02:27\",\"ident\":\"dnsmasq\",\"owner\":\"client01\",\"log_format\":\"dnsmasq\",\"pid\":\"3468\",\"message\":\"query[A] db.local.clamav.net from 172.19.130.4\"}'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"KInesisTest\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"ACCESS_KEY\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"SECRET_ACCESS_KEY\") \\\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnsmasq_regex = r\"([a-z]+\\[?[A-Z]*\\]?)\\s(\\S+)\\s([fromtois]+)\\s(\\S+)\"\n",
    "indexer_dnsmasq = StringIndexerModel.read().load(\"./model/indexer_dnsmasq\")\n",
    "indexer_error = StringIndexerModel.read().load(\"./model/indexer_apacheerror\")\n",
    "def process_dnsmasq(filtered_rdd):\n",
    "    #fix timestamp, create key, create value\n",
    "    df=spark.createDataFrame(filtered_rdd)\n",
    "    df=df.withColumn(\"response\", regexp_extract(\"message\", dnsmasq_regex, 1)) \\\n",
    "    .withColumn(\"domain\", regexp_extract(\"message\", dnsmasq_regex, 2)) \\\n",
    "    .withColumn(\"term\", regexp_extract(\"message\", dnsmasq_regex, 3)) \\\n",
    "    .withColumn(\"ip_addr\", regexp_extract(\"message\", dnsmasq_regex, 4)) \\\n",
    "    .withColumn(\"key\", concat(\"response\", lit(\" <*> \"),\"term\", lit(\" <*>\"))) \\\n",
    "    .withColumn(\"value1\", regexp_extract(\"message\", dnsmasq_regex, 2)) \\\n",
    "    .withColumn(\"value2\", regexp_extract(\"message\", dnsmasq_regex, 4)) \\\n",
    "    .withColumn(\"time\", concat(\"time\", lit(\" 2022\"))) \\\n",
    "    .withColumn(\"epoch_timestamp\", unix_timestamp(\"time\", \"MMM dd HH:mm:ss yyyy\")) \n",
    "    df_indexed=indexer_dnsmasq.transform(df)\n",
    "    df_indexed=df_indexed.drop(\"pid\",\"time\",\"response\",\"domain\",\"term\",\"ip_addr\")\n",
    "    return df_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_ip(value2):\n",
    "    try:\n",
    "        # Split IP address into octets\n",
    "        octets = list(map(int, value2.split('.')))\n",
    "\n",
    "        # Check if IP is private\n",
    "        if (octets[0] == 10) or (octets[0] == 172 and 16 <= octets[1] <= 31) or (octets[0] == 192 and octets[1] == 168):\n",
    "            return 0  # Private IP\n",
    "        else:\n",
    "            return 1  # Public IP\n",
    "    except:\n",
    "        return 2  # Non-IP values\n",
    "    \n",
    "# Custom function to count dots\n",
    "def count_dots(s):\n",
    "    return s.count('.')\n",
    "\n",
    "# Custom function to count hyphens\n",
    "def count_hyphens(s):\n",
    "    return s.count('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dnsmasq_for_pred(df_pyspark):\n",
    "    # df_pyspark = df_pyspark.drop(\"ident\", \"log_format\", \"owner\", \"message\")  # Remove unnecessary columns\n",
    "    df_pyspark = df_pyspark.withColumn(\"encoded_key\", col(\"encoded_key\").cast(\"int\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"timestamp_datetime\", from_unixtime(\"epoch_timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "    lag_col = lag(col(\"epoch_timestamp\")).over(Window.orderBy(\"epoch_timestamp\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"time_diff_unix\", round((col(\"epoch_timestamp\") - lag_col), 1))\n",
    "    df_pyspark = df_pyspark.fillna(0, subset=[\"time_diff_unix\"])\n",
    "    df_pyspark = df_pyspark.withColumn(\"time_diff_unix\", col(\"time_diff_unix\").cast(\"decimal(10,1)\"))\n",
    "\n",
    "    df_pyspark = df_pyspark.withColumn(\"day_of_week\", dayofweek(\"timestamp_datetime\"))\n",
    "    \n",
    "    df_pyspark = df_pyspark.withColumn(\"value1_length\", length(\"value1\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"value2_length\", length(\"value2\"))\n",
    "\n",
    "    count_dots_udf = udf(count_dots, IntegerType())\n",
    "    count_hyphens_udf = udf(count_hyphens, IntegerType())\n",
    "    \n",
    "\n",
    "    ##### new \n",
    "\n",
    "    # Apply UDFs to DataFrame\n",
    "    df_pyspark = df_pyspark.withColumn(\"value1_dot_count\", count_dots_udf(\"value1\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"value1_hyphen_count\", count_hyphens_udf(\"value1\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"value2_dot_count\", count_dots_udf(\"value2\"))\n",
    "    df_pyspark = df_pyspark.withColumn(\"value2_hyphen_count\", count_hyphens_udf(\"value2\"))\n",
    "\n",
    "    ##### new \n",
    "\n",
    "    df_pyspark = df_pyspark.withColumn(\"key_length\", length(\"key\"))\n",
    "\n",
    "    window_spec_value1 = Window().orderBy(\"value1\")\n",
    "    window_spec_value2 = Window().orderBy(\"value2\")\n",
    "    \n",
    "    df_pyspark = df_pyspark.withColumn(\"value1_count\", count(\"value1\").over(window_spec_value1))\n",
    "    df_pyspark = df_pyspark.withColumn(\"value2_count\", count(\"value2\").over(window_spec_value2))\n",
    "\n",
    "    # Register the UDF\n",
    "    categorize_ip_udf = udf(categorize_ip, IntegerType())\n",
    "    \n",
    "    # Apply the UDF to create a new column 'ip_category'\n",
    "    df_pyspark = df_pyspark.withColumn(\"value2_ip_class\", categorize_ip_udf(\"value2\"))\n",
    "    \n",
    "    return df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_rdd(rdd):\n",
    "    print(\"enter check\")\n",
    "    start_time_process = time.time()\n",
    "    if not rdd.isEmpty():\n",
    "        print(\"not empty\")\n",
    "        dataframes = {}\n",
    "        json_rdd = rdd.map(lambda x: json.loads(x))\n",
    "        apache_error = json_rdd.filter(lambda x: x.get('log_format') == 'apache_error')\n",
    "        dnsmasq_rdd = json_rdd.filter(lambda x: x.get('log_format') == 'dnsmasq')\n",
    "        apache2_access=json_rdd.filter(lambda x:x.get('log_format')==\"apache2_access\")\n",
    "        if not dnsmasq_rdd.isEmpty():\n",
    "            print(\"***** phase 1 dnsmasq log seperation ******\")\n",
    "            \n",
    "            start_time_dns = time.time()\n",
    "\n",
    "            dataframes['dnsmasq'] = process_dnsmasq(dnsmasq_rdd)\n",
    "            # dataframes['dnsmasq'].show()\n",
    "            unique_owners=dataframes['dnsmasq'].select('owner').distinct()\n",
    "            \n",
    "            end_time_dns = time.time()\n",
    "\n",
    "            elapsed_time = end_time_dns - start_time_dns\n",
    "            print(\"Pre-processed time <dnsmasq_phase_1>:\", elapsed_time, \"seconds\\n\")\n",
    "            log_model = joblib.load('C:\\\\Users\\\\A570ZD\\\\Desktop\\\\siem dev2\\\\model\\\\ML_trained_model\\\\RandomForestClassifier2.joblib')\n",
    "            \n",
    "            log_model.feature_names = None\n",
    "\n",
    "            broadcast_model = sc.broadcast(log_model)\n",
    "            @udf('integer')\n",
    "            def predict_data(*cols):\n",
    "                return int(broadcast_model.value.predict((cols,)))\n",
    "            \n",
    "            for row in unique_owners.collect():\n",
    "                    print(\"***** phase 2 dnsmasq owner seperation & furthur pre-processing ******\")\n",
    "\n",
    "                    # since only 1 column is collected , so it's always at row[0]\n",
    "                    unique_value = row[0]\n",
    "                    df_temp = dataframes['dnsmasq'].filter(dataframes['dnsmasq']['owner'] == unique_value)\n",
    "                    owner = df_temp.select(\"owner\").first()[0]\n",
    "                    # df_temp.show()\n",
    "\n",
    "                    df_temp = process_dnsmasq_for_pred(df_temp)  \n",
    "                    # df_temp.show() \n",
    "\n",
    "                    end_time_dns = time.time()\n",
    "\n",
    "                    elapsed_time = end_time_dns - start_time_dns\n",
    "                    print(f\"Pre-processed time <dnsmasq_phase_2_{owner}>:\", elapsed_time, \"seconds\\n\")\n",
    "                    \n",
    "                    print(\"***** phase 3 dnsmasq  rule-base detection ******\")\n",
    "                    start_time_dns = time.time()\n",
    "\n",
    "                    ##### code for rule based prediction\n",
    "                    print(\"XXXX nothing XXXX\")\n",
    "\n",
    " \n",
    "\n",
    "                    # send to anomaly module\n",
    "                    print(\"***** phase 4 dnsmasq  anomaly detection ******\")\n",
    "                    \n",
    "\n",
    "                    df_pyspark = df_temp.alias(\"df_pyspark\")\n",
    "                    df_pyspark = df_pyspark.drop(\"key\", \"value1\", \"value2\", \"epoch_timestamp\",\"timestamp_datetime\")\n",
    "                    df_pyspark = df_pyspark.selectExpr(\n",
    "                        'encoded_key',\n",
    "                        'time_diff_unix',\n",
    "                        'day_of_week',\n",
    "                        'value1_length',\n",
    "                        'value2_length',\n",
    "                        'value1_dot_count',\n",
    "                        'value1_hyphen_count',\n",
    "                        'value2_dot_count',\n",
    "                        'value2_hyphen_count',\n",
    "                        'key_length',\n",
    "                        'value1_count',\n",
    "                        'value2_count',\n",
    "                        'value2_ip_class'  # Assuming this column needs to be added\n",
    "                    )\n",
    "\n",
    "                    list_of_columns = df_pyspark.columns\n",
    "                    df_pyspark = df_pyspark.withColumn(\"prediction\", predict_data(*list_of_columns))\n",
    "                    prediction_column=df_pyspark.select(\"prediction\")\n",
    "                    # df_temp=df_temp.join(prediction_column)\n",
    "                    w = Window().orderBy(lit('A'))\n",
    "                    prediction_column = prediction_column.withColumn('id', row_number().over(w))\n",
    "                    df_temp = df_temp.withColumn('id', row_number().over(w))\n",
    "\n",
    "                    #join together both DataFrames using 'id' column\n",
    "                    df_temp = df_temp.join(prediction_column, on=['id']).drop('id')\n",
    "                    df_temp=df_temp.select('message','value1',\"value2\",\"prediction\")\n",
    "                    df_pyspark.show()\n",
    "\n",
    "                    end_time_dns = time.time()\n",
    "\n",
    "                    elapsed_time = end_time_dns - start_time_dns\n",
    "                    \n",
    "                    print(f\"Anomaly detection time <dnsmasq_phase_4_{owner}>:\", elapsed_time, \"seconds\\n\")\n",
    "                    print(\"******** phase 5 rule creation **********\")\n",
    "                    \n",
    "\n",
    "\n",
    "                    ########################\n",
    "                    print(\"************ END ***********************************\") \n",
    "                    return df_temp\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter check\n",
      "not empty\n",
      "***** phase 1 dnsmasq log seperation ******\n",
      "Pre-processed time <dnsmasq_phase_1>: 2.8307573795318604 seconds\n",
      "\n",
      "***** phase 2 dnsmasq owner seperation & furthur pre-processing ******\n",
      "Pre-processed time <dnsmasq_phase_2_client01>: 24.80426526069641 seconds\n",
      "\n",
      "***** phase 3 dnsmasq  rule-base detection ******\n",
      "XXXX nothing XXXX\n",
      "***** phase 4 dnsmasq  anomaly detection ******\n",
      "+-----------+--------------+-----------+-------------+-------------+----------------+-------------------+----------------+-------------------+----------+------------+------------+---------------+----------+\n",
      "|encoded_key|time_diff_unix|day_of_week|value1_length|value2_length|value1_dot_count|value1_hyphen_count|value2_dot_count|value2_hyphen_count|key_length|value1_count|value2_count|value2_ip_class|prediction|\n",
      "+-----------+--------------+-----------+-------------+-------------+----------------+-------------------+----------------+-------------------+----------+------------+------------+---------------+----------+\n",
      "|          0|           0.0|          6|           22|           42|               3|                  0|               2|                  0|        16|          13|           1|              2|         0|\n",
      "|          2|          13.0|          6|          203|           12|              10|                  8|               3|                  0|        21|           4|           4|              0|         1|\n",
      "|          2|          19.0|          6|          203|           12|              10|                  8|               3|                  0|        21|           7|           4|              0|         1|\n",
      "|          2|          15.0|          6|          203|           12|              10|                  8|               3|                  0|        21|          10|           4|              0|         1|\n",
      "|          7|           9.0|          6|           22|           12|               3|                  0|               3|                  0|        23|          13|           6|              0|         0|\n",
      "|          2|           0.0|          6|           19|           12|               3|                  0|               3|                  0|        21|          14|           6|              0|         0|\n",
      "|          1|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        20|           4|          10|              0|         1|\n",
      "|          1|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        20|           7|          10|              0|         1|\n",
      "|          1|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        20|          10|          10|              0|         1|\n",
      "|          1|           0.0|          6|           22|           15|               3|                  0|               3|                  0|        20|          13|          10|              0|         0|\n",
      "|          0|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        16|           1|          14|              1|         1|\n",
      "|          0|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        16|           4|          14|              1|         1|\n",
      "|          0|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        16|           7|          14|              1|         1|\n",
      "|          0|           0.0|          6|          203|           15|              10|                  8|               3|                  0|        16|          10|          14|              1|         1|\n",
      "+-----------+--------------+-----------+-------------+-------------+----------------+-------------------+----------------+-------------------+----------+------------+------------+---------------+----------+\n",
      "\n",
      "Anomaly detection time <dnsmasq_phase_4_client01>: 30.202549934387207 seconds\n",
      "\n",
      "******** phase 5 rule creation **********\n",
      "************ END ***********************************\n"
     ]
    }
   ],
   "source": [
    "df_anomaly_extraact=process_rdd(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|             message|              value1|              value2|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|reply 3x6-.601-.T...|3x6-.601-.TtfFB8j...|     195.128.194.168|         0|\n",
      "|query[A] 3x6-.602...|3x6-.602-.OgEWPzR...|        10.143.0.103|         1|\n",
      "|forwarded 3x6-.60...|3x6-.602-.OgEWPzR...|     192.168.231.254|         1|\n",
      "|reply 3x6-.602-.O...|3x6-.602-.OgEWPzR...|     195.128.194.168|         1|\n",
      "|query[A] 3x6-.603...|3x6-.603-.JAiihuf...|        10.143.0.103|         0|\n",
      "|forwarded 3x6-.60...|3x6-.603-.JAiihuf...|     192.168.231.254|         0|\n",
      "|reply 3x6-.603-.J...|3x6-.603-.JAiihuf...|     195.128.194.168|         1|\n",
      "|query[A] 3x6-.604...|3x6-.604-.MjQIsKR...|        10.143.0.103|         1|\n",
      "|forwarded 3x6-.60...|3x6-.604-.MjQIsKR...|     192.168.231.254|         1|\n",
      "|reply 3x6-.604-.M...|3x6-.604-.MjQIsKR...|     195.128.194.168|         0|\n",
      "|query[TXT] curren...|current.cvd.clama...|        172.19.130.4|         1|\n",
      "|forwarded current...|current.cvd.clama...|     192.168.231.254|         1|\n",
      "|reply current.cvd...|current.cvd.clama...|0.103.5:62:26428:...|         1|\n",
      "|query[A] db.local...| db.local.clamav.net|        172.19.130.4|         1|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_anomaly_extraact.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_df=df_anomaly_extraact[df_anomaly_extraact['prediction']==1].drop('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             message|              value1|              value2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|query[A] 3x6-.602...|3x6-.602-.OgEWPzR...|        10.143.0.103|\n",
      "|forwarded 3x6-.60...|3x6-.602-.OgEWPzR...|     192.168.231.254|\n",
      "|reply 3x6-.602-.O...|3x6-.602-.OgEWPzR...|     195.128.194.168|\n",
      "|reply 3x6-.603-.J...|3x6-.603-.JAiihuf...|     195.128.194.168|\n",
      "|query[A] 3x6-.604...|3x6-.604-.MjQIsKR...|        10.143.0.103|\n",
      "|forwarded 3x6-.60...|3x6-.604-.MjQIsKR...|     192.168.231.254|\n",
      "|query[TXT] curren...|current.cvd.clama...|        172.19.130.4|\n",
      "|forwarded current...|current.cvd.clama...|     192.168.231.254|\n",
      "|reply current.cvd...|current.cvd.clama...|0.103.5:62:26428:...|\n",
      "|query[A] db.local...| db.local.clamav.net|        172.19.130.4|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rule_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
